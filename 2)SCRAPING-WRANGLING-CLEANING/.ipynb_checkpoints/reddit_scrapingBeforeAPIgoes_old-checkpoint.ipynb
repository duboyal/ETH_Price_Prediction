{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import prawcore\n",
    "\n",
    "reddit = praw.Reddit(client_id = 'GeSMulGZbc7PeQ', client_secret = 'uSFY4s3PyufZwc9oHpIVlXpgI5Q', user_agent = 'scrapedata5')\n",
    "\n",
    "subreddit1 = reddit.subreddit('electroneum')\n",
    "submissions = subreddit1.submissions(1517619814,1517965414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddits = []\n",
    "for submission in submissions:\n",
    "    post_id = submission.id\n",
    "    post_title = submission.title if submission.title else ''\n",
    "    # assuming it has a body\n",
    "    post_body = submission.selftext if submission.selftext else ''\n",
    "    post_text = post_title + ' ' + post_body\n",
    "    post_time = submission.created\n",
    "    post_type = 'post'\n",
    "    post_doc = {'doc_id': post_id,\n",
    "                'text': post_text,\n",
    "                'timestamp': post_time,\n",
    "                'doc_type': 'post'}\n",
    "    \n",
    "    reddits.append(post_doc)\n",
    "    \n",
    "    #we append to the main list first, then deal with the nesting\n",
    "    \n",
    "    top_level_comments = [comm for comm in submission.comments]\n",
    "    if top_level_comments:\n",
    "        for tlc in top_level_comments:\n",
    "            post_text = tlc.body \n",
    "            post_time = tlc.created\n",
    "            post_doc = {'doc_id': post_id,\n",
    "                        'text': post_text,\n",
    "                        'timestamp': post_time,\n",
    "                        'doc_type': 'comment'}\n",
    "            reddits.append(post_doc)\n",
    "            second_level_comments = [comm for comm in tlc.replies]\n",
    "            if second_level_comments:\n",
    "                for slc in second_level_comments:\n",
    "                    post_text = slc.body \n",
    "                    post_time = slc.created\n",
    "                    post_doc = {'doc_id': post_id,\n",
    "                                'text': post_text,\n",
    "                                'timestamp': post_time,\n",
    "                                'doc_type': 'comment'}\n",
    "                    reddits.append(post_doc)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(reddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7vrrys</td>\n",
       "      <td>post</td>\n",
       "      <td>Hes said HODL! https://cointelegraph.com/news/...</td>\n",
       "      <td>1.517989e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7vrive</td>\n",
       "      <td>post</td>\n",
       "      <td>The reason why Singapore will not ban cryptocu...</td>\n",
       "      <td>1.517987e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7vr8nd</td>\n",
       "      <td>post</td>\n",
       "      <td>Privateview key and secret key in online or mo...</td>\n",
       "      <td>1.517985e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7vqxew</td>\n",
       "      <td>post</td>\n",
       "      <td>Video Tutorial - How to Update to v0.11.1.0</td>\n",
       "      <td>1.517982e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7vqxew</td>\n",
       "      <td>comment</td>\n",
       "      <td>STOP FREAKING OUT!\\nThis is Crypto Sewer's gui...</td>\n",
       "      <td>1.517984e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7vqxew</td>\n",
       "      <td>comment</td>\n",
       "      <td>Thanks dude, appreciate it.</td>\n",
       "      <td>1.518138e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7vqxew</td>\n",
       "      <td>comment</td>\n",
       "      <td>You don't need to sync before updating, but yo...</td>\n",
       "      <td>1.518228e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>post</td>\n",
       "      <td>Electroneum Update Email (6/2/18) Check your s...</td>\n",
       "      <td>1.517979e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>Mobile mining has to be in place for MWC.</td>\n",
       "      <td>1.517984e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>All good. Hope we'll get into bull run before ...</td>\n",
       "      <td>1.517985e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>Me too....someone let the BULLS out</td>\n",
       "      <td>1.518007e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>What actually has to be tested with the \"mobil...</td>\n",
       "      <td>1.518012e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>People aren't making of a deal on Mobile World...</td>\n",
       "      <td>1.518001e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>KuCoin is nice.. it will be awesome if it gets...</td>\n",
       "      <td>1.518013e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>In a bull market these events would probably h...</td>\n",
       "      <td>1.518007e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>Great mail and I think GSMA combine with MWC e...</td>\n",
       "      <td>1.517988e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>I'm hyped for the Mobile Mining. Since I don't...</td>\n",
       "      <td>1.518008e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7vqjc3</td>\n",
       "      <td>comment</td>\n",
       "      <td>I didnâ€™t receive any email!</td>\n",
       "      <td>1.518024e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7vqai3</td>\n",
       "      <td>post</td>\n",
       "      <td>error code 16 hello! i want to transfer my ele...</td>\n",
       "      <td>1.517978e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7vqai3</td>\n",
       "      <td>comment</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1.517978e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id doc_type                                               text  \\\n",
       "0   7vrrys     post  Hes said HODL! https://cointelegraph.com/news/...   \n",
       "1   7vrive     post  The reason why Singapore will not ban cryptocu...   \n",
       "2   7vr8nd     post  Privateview key and secret key in online or mo...   \n",
       "3   7vqxew     post       Video Tutorial - How to Update to v0.11.1.0    \n",
       "4   7vqxew  comment  STOP FREAKING OUT!\\nThis is Crypto Sewer's gui...   \n",
       "5   7vqxew  comment                       Thanks dude, appreciate it.    \n",
       "6   7vqxew  comment  You don't need to sync before updating, but yo...   \n",
       "7   7vqjc3     post  Electroneum Update Email (6/2/18) Check your s...   \n",
       "8   7vqjc3  comment         Mobile mining has to be in place for MWC.    \n",
       "9   7vqjc3  comment  All good. Hope we'll get into bull run before ...   \n",
       "10  7vqjc3  comment                Me too....someone let the BULLS out   \n",
       "11  7vqjc3  comment  What actually has to be tested with the \"mobil...   \n",
       "12  7vqjc3  comment  People aren't making of a deal on Mobile World...   \n",
       "13  7vqjc3  comment  KuCoin is nice.. it will be awesome if it gets...   \n",
       "14  7vqjc3  comment  In a bull market these events would probably h...   \n",
       "15  7vqjc3  comment  Great mail and I think GSMA combine with MWC e...   \n",
       "16  7vqjc3  comment  I'm hyped for the Mobile Mining. Since I don't...   \n",
       "17  7vqjc3  comment                        I didnâ€™t receive any email!   \n",
       "18  7vqai3     post  error code 16 hello! i want to transfer my ele...   \n",
       "19  7vqai3  comment                                          [deleted]   \n",
       "\n",
       "       timestamp  \n",
       "0   1.517989e+09  \n",
       "1   1.517987e+09  \n",
       "2   1.517985e+09  \n",
       "3   1.517982e+09  \n",
       "4   1.517984e+09  \n",
       "5   1.518138e+09  \n",
       "6   1.518228e+09  \n",
       "7   1.517979e+09  \n",
       "8   1.517984e+09  \n",
       "9   1.517985e+09  \n",
       "10  1.518007e+09  \n",
       "11  1.518012e+09  \n",
       "12  1.518001e+09  \n",
       "13  1.518013e+09  \n",
       "14  1.518007e+09  \n",
       "15  1.517988e+09  \n",
       "16  1.518008e+09  \n",
       "17  1.518024e+09  \n",
       "18  1.517978e+09  \n",
       "19  1.517978e+09  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Alexandra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d534546049c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'words.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "para = \"What can I say about this place. The staff of the restaurant is nice and the eggplant is not bad. Apart from that, very uninspired food, lack of atmosphere and too expensive. I am a staunch vegetarian and was sorely dissapointed with the veggie options on the menu. Will be the last time I visit, I recommend others to avoid\"\n",
    "\n",
    "sentense = word_tokenize(para)\n",
    "word_features = []\n",
    "\n",
    "for i,j in nltk.pos_tag(sentense):\n",
    "    if j in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']: \n",
    "        word_features.append(i)\n",
    "\n",
    "rating = 0\n",
    "\n",
    "for i in word_features:\n",
    "    with open('words.txt', 'rt') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            if i == row[0]:\n",
    "                print(i, row[1])\n",
    "                if row[1] == 'pos':\n",
    "                    rating = rating + 1\n",
    "                elif row[1] == 'neg':\n",
    "                    rating = rating - 1\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fc62de1987ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtext_normalizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Alexandra/Desktop/sentAnalysis_onMyData/text_normalizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# # Import necessary dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoktok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToktokTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import text_normalizer as tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader_lexicon(review, \n",
    "                                    threshold=0.1,\n",
    "                                    verbose=False):\n",
    "    # pre-process text\n",
    "    review = tn.strip_html_tags(review)\n",
    "    review = tn.remove_accented_chars(review)\n",
    "    review = tn.expand_contractions(review)\n",
    "    \n",
    "    # analyze the sentiment for review\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    # get aggregate scores and final sentiment\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 'positive' if agg_score >= threshold\\\n",
    "                                   else 'negative'\n",
    "    if verbose:\n",
    "        # display detailed sentiment statistics\n",
    "        positive = str(round(scores['pos'], 2)*100)+'%'\n",
    "        final = round(agg_score, 2)\n",
    "        negative = str(round(scores['neg'], 2)*100)+'%'\n",
    "        neutral = str(round(scores['neu'], 2)*100)+'%'\n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n",
    "                                        negative, neutral]],\n",
    "                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Polarity Score',\n",
    "                                                                       'Positive', 'Negative', 'Neutral']], \n",
    "                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print(sentiment_frame)\n",
    "    \n",
    "    return final_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e9535a50afc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_sentiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_vader_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "predicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
